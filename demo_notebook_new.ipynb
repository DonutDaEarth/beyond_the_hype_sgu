{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad73f230",
   "metadata": {},
   "source": [
    "# Beyond the Hype: When GPT-2 Hallucinates\n",
    "\n",
    "## Demonstrating Factual Inaccuracies in AI-Generated Content\n",
    "\n",
    "**Project Thesis:** Despite GPT-2's impressive text generation capabilities, the model frequently generates plausible-sounding but factually incorrect or nonsensical information (hallucinations), making it unreliable for tasks requiring factual accuracy like summarization, translation, and information retrieval.\n",
    "\n",
    "**Key Limitation:** Factual Inaccuracy (Hallucination)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2416a",
   "metadata": {},
   "source": [
    "## Setup: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb37d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "%pip install transformers torch pandas matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779cd1d",
   "metadata": {},
   "source": [
    "## Load GPT-2 Model\n",
    "\n",
    "We'll use **GPT-2** (OpenAI, 2019) - a 117M parameter autoregressive language model.\n",
    "\n",
    "**Why GPT-2?**\n",
    "- Widely used for text generation tasks\n",
    "- Known for producing fluent, coherent text\n",
    "- **Critical flaw:** Frequently generates plausible-sounding but factually incorrect information (hallucinations)\n",
    "\n",
    "**The Problem:** When GPT-2 generates text, it's predicting the most likely next word based on patterns in training data, NOT checking facts or maintaining consistency with reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading GPT-2 model...\\n\")\n",
    "\n",
    "# Load GPT-2\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2_model.eval()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úì GPT-2 Model Loaded Successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìä Model Details:\")\n",
    "print(f\"  ‚Ä¢ Model: GPT-2\")\n",
    "print(f\"  ‚Ä¢ Parameters: 117 million\")\n",
    "print(f\"  ‚Ä¢ Developer: OpenAI (2019)\")\n",
    "print(f\"  ‚Ä¢ Architecture: Transformer decoder-only\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d918148",
   "metadata": {},
   "source": [
    "## Helper Functions for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65959a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt2_text(prompt, max_length=200, temperature=0.8, num_return=1):\n",
    "    \"\"\"Generate text using GPT-2 model\"\"\"\n",
    "    input_ids = gpt2_tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return,\n",
    "            pad_token_id=gpt2_tokenizer.eos_token_id,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    \n",
    "    return gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def display_hallucination_comparison(test_name, prompt, ground_truth, gpt2_output, hallucinations):\n",
    "    \"\"\"Display a formatted comparison highlighting hallucinations\"\"\"\n",
    "    html = f\"\"\"\n",
    "    <div style='background-color: #f0f0f0; padding: 20px; border-radius: 10px; margin: 10px 0; border: 2px solid #95a5a6;'>\n",
    "        <h3 style='color: #2c3e50;'>üß™ Test Case: {test_name}</h3>\n",
    "        <div style='background-color: white; color: #2c3e50; padding: 15px; margin: 10px 0; border-left: 4px solid #3498db;'>\n",
    "            <strong style='font-size: 16px;'>üìù Prompt:</strong><br>\n",
    "            <span style='font-size: 16px; color: #2c3e50;'>{prompt}</span>\n",
    "        </div>\n",
    "        <div style='background-color: #d4edda; color: #155724; padding: 15px; margin: 10px 0; border-left: 4px solid #28a745;'>\n",
    "            <strong style='font-size: 16px;'>‚úì Ground Truth (Actual Facts):</strong><br>\n",
    "            <span style='font-size: 16px; color: #155724;'>{ground_truth}</span>\n",
    "        </div>\n",
    "        <div style='background-color: #f8d7da; color: #721c24; padding: 15px; margin: 10px 0; border-left: 4px solid #dc3545; border: 2px solid #dc3545;'>\n",
    "            <strong style='font-size: 16px;'>‚ùå GPT-2 Output (Contains Hallucinations):</strong><br>\n",
    "            <span style='font-size: 16px; color: #721c24;'>{gpt2_output}</span>\n",
    "        </div>\n",
    "        <div style='background-color: #fff3cd; color: #856404; padding: 15px; margin: 10px 0; border-left: 4px solid #ffc107;'>\n",
    "            <strong style='font-size: 16px;'>üö® Identified Hallucinations:</strong><br>\n",
    "            <ul style='margin: 10px 0; font-size: 15px;'>\n",
    "    \"\"\"\n",
    "    \n",
    "    for hallucination in hallucinations:\n",
    "        html += f\"<li style='margin: 5px 0; color: #856404;'><strong>{hallucination}</strong></li>\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html))\n",
    "\n",
    "print(\"‚úì Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa3de44",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ DEMONSTRATION 1: Historical Facts Hallucination\n",
    "\n",
    "**Task:** Generate text about historical events\n",
    "\n",
    "**Risk:** The model will confidently state incorrect dates, names, and events that sound plausible but are completely fabricated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: World War II End Date\n",
    "prompt1 = \"World War II ended in\"\n",
    "\n",
    "gpt2_output1 = generate_gpt2_text(prompt1, max_length=100, temperature=0.7)\n",
    "\n",
    "ground_truth1 = \"World War II ended in 1945 (May 8, 1945 in Europe, September 2, 1945 in the Pacific)\"\n",
    "\n",
    "hallucinations1 = [\n",
    "    \"Check if GPT-2 states a different year\",\n",
    "    \"Check for invented battles or events\",\n",
    "    \"Check for incorrect surrender details\",\n",
    "    \"Check for fabricated historical figures\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Historical Event - WWII End Date\",\n",
    "    prompt1,\n",
    "    ground_truth1,\n",
    "    gpt2_output1,\n",
    "    hallucinations1\n",
    ")\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### üîç Analysis\n",
    "GPT-2 generates text based on statistical patterns, not factual knowledge. Even for well-documented historical events, it may:\n",
    "- Generate plausible-sounding but incorrect dates\n",
    "- Invent non-existent historical details\n",
    "- Confuse different events or time periods\n",
    "- Mix facts from different contexts\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6bce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Scientific Discovery\n",
    "prompt2 = \"Albert Einstein discovered the theory of relativity in\"\n",
    "\n",
    "gpt2_output2 = generate_gpt2_text(prompt2, max_length=120, temperature=0.7)\n",
    "\n",
    "ground_truth2 = \"Albert Einstein published the special theory of relativity in 1905 and the general theory of relativity in 1915\"\n",
    "\n",
    "hallucinations2 = [\n",
    "    \"Check for incorrect year\",\n",
    "    \"Check for invented collaborators or universities\",\n",
    "    \"Check for fabricated discovery details\",\n",
    "    \"Check for incorrect scientific claims\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Scientific Discovery - Theory of Relativity\",\n",
    "    prompt2,\n",
    "    ground_truth2,\n",
    "    gpt2_output2,\n",
    "    hallucinations2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8cb40d",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ DEMONSTRATION 2: Geographic Information Hallucination\n",
    "\n",
    "**Task:** Complete sentences about geography\n",
    "\n",
    "**Risk:** GPT-2 will fabricate capitals, populations, locations, and geographic features that don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c65da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Country Capital\n",
    "prompt3 = \"The capital of Australia is\"\n",
    "\n",
    "gpt2_output3 = generate_gpt2_text(prompt3, max_length=80, temperature=0.7)\n",
    "\n",
    "ground_truth3 = \"The capital of Australia is Canberra (not Sydney or Melbourne)\"\n",
    "\n",
    "hallucinations3 = [\n",
    "    \"Check if it states Sydney or Melbourne (common misconception)\",\n",
    "    \"Check for invented city names\",\n",
    "    \"Check for incorrect population figures\",\n",
    "    \"Check for fabricated historical details\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Geographic Fact - Australian Capital\",\n",
    "    prompt3,\n",
    "    ground_truth3,\n",
    "    gpt2_output3,\n",
    "    hallucinations3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Mountain Location\n",
    "prompt4 = \"Mount Everest is located in\"\n",
    "\n",
    "gpt2_output4 = generate_gpt2_text(prompt4, max_length=100, temperature=0.7)\n",
    "\n",
    "ground_truth4 = \"Mount Everest is located on the border between Nepal and Tibet (China), with a height of 8,848.86 meters\"\n",
    "\n",
    "hallucinations4 = [\n",
    "    \"Check for incorrect location\",\n",
    "    \"Check for fabricated height measurements\",\n",
    "    \"Check for invented surrounding mountains\",\n",
    "    \"Check for false climbing records\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Geographic Fact - Mount Everest\",\n",
    "    prompt4,\n",
    "    ground_truth4,\n",
    "    gpt2_output4,\n",
    "    hallucinations4\n",
    ")\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### üí° Key Insight #1\n",
    "GPT-2 generates text that **sounds plausible** but lacks grounding in factual knowledge. The model:\n",
    "- Doesn't \"know\" facts - it predicts likely word sequences\n",
    "- Can't distinguish between true and false information\n",
    "- Confidently generates incorrect information\n",
    "- Produces fluent but factually wrong text\n",
    "\n",
    "**This makes it unreliable for any factual information tasks!**\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8710a4f",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ DEMONSTRATION 3: News Summarization Hallucination\n",
    "\n",
    "**Task:** Summarize news articles\n",
    "\n",
    "**Risk:** GPT-2 will add details that weren't in the original text, change key facts, or invent quotes and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: News Article Summarization\n",
    "news_article = \"\"\"A breakthrough study published in Nature revealed that scientists at MIT \n",
    "have developed a new battery technology that could extend electric vehicle range by 50%. \n",
    "The research team, led by Dr. Sarah Chen, tested the batteries over 1,000 charge cycles. \n",
    "The technology is expected to be commercially available by 2026.\"\"\"\n",
    "\n",
    "prompt5 = news_article + \"\\n\\nTL;DR:\"\n",
    "\n",
    "gpt2_output5 = generate_gpt2_text(prompt5, max_length=250, temperature=0.7)\n",
    "\n",
    "ground_truth5 = \"MIT scientists developed battery tech for 50% more EV range, tested over 1,000 cycles, available by 2026\"\n",
    "\n",
    "hallucinations5 = [\n",
    "    \"Check for changed percentages or numbers\",\n",
    "    \"Check for invented researcher names or institutions\",\n",
    "    \"Check for fabricated quotes or statements\",\n",
    "    \"Check for added details not in original text\",\n",
    "    \"Check for changed dates or timelines\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"News Summarization - Battery Technology\",\n",
    "    \"Summarize the news article\",\n",
    "    ground_truth5,\n",
    "    gpt2_output5,\n",
    "    hallucinations5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb446e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Medical Research Summarization\n",
    "medical_article = \"\"\"A clinical trial involving 500 patients found that a new drug reduced \n",
    "symptoms of arthritis by 35% compared to placebo. The study was conducted over 12 months \n",
    "at Johns Hopkins Hospital. Side effects were minimal and included mild headaches in 8% of participants.\"\"\"\n",
    "\n",
    "prompt6 = medical_article + \"\\n\\nSummary:\"\n",
    "\n",
    "gpt2_output6 = generate_gpt2_text(prompt6, max_length=220, temperature=0.7)\n",
    "\n",
    "ground_truth6 = \"Clinical trial with 500 patients showed 35% symptom reduction, 12 months at Johns Hopkins, 8% had mild headaches\"\n",
    "\n",
    "hallucinations6 = [\n",
    "    \"Check for incorrect patient numbers\",\n",
    "    \"Check for changed effectiveness percentages\",\n",
    "    \"Check for invented side effects\",\n",
    "    \"Check for wrong hospital or location\",\n",
    "    \"Check for fabricated medical claims\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Medical Summary - Arthritis Drug Trial\",\n",
    "    \"Summarize the medical research\",\n",
    "    ground_truth6,\n",
    "    gpt2_output6,\n",
    "    hallucinations6\n",
    ")\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### üö® Critical Risk in Summarization\n",
    "When used for summarization, GPT-2:\n",
    "- **Adds information** that wasn't in the source\n",
    "- **Changes numerical values** (percentages, dates, quantities)\n",
    "- **Invents quotes** or attributions\n",
    "- **Alters key facts** while maintaining fluent language\n",
    "\n",
    "**This is especially dangerous in medical, legal, or financial contexts!**\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7088c29",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ DEMONSTRATION 4: Question Answering Hallucination\n",
    "\n",
    "**Task:** Answer factual questions\n",
    "\n",
    "**Risk:** GPT-2 will provide confident but incorrect answers to straightforward questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e145c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7: Simple Factual Question\n",
    "prompt7 = \"Q: How many continents are there on Earth?\\nA:\"\n",
    "\n",
    "gpt2_output7 = generate_gpt2_text(prompt7, max_length=80, temperature=0.7)\n",
    "\n",
    "ground_truth7 = \"There are 7 continents: Africa, Antarctica, Asia, Europe, North America, Australia (Oceania), and South America\"\n",
    "\n",
    "hallucinations7 = [\n",
    "    \"Check for incorrect number of continents\",\n",
    "    \"Check for invented continent names\",\n",
    "    \"Check for missing actual continents\",\n",
    "    \"Check for nonsensical geographic claims\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Question Answering - Number of Continents\",\n",
    "    prompt7,\n",
    "    ground_truth7,\n",
    "    gpt2_output7,\n",
    "    hallucinations7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 8: Company Information\n",
    "prompt8 = \"Q: When was Microsoft founded?\\nA:\"\n",
    "\n",
    "gpt2_output8 = generate_gpt2_text(prompt8, max_length=90, temperature=0.7)\n",
    "\n",
    "ground_truth8 = \"Microsoft was founded on April 4, 1975, by Bill Gates and Paul Allen in Albuquerque, New Mexico\"\n",
    "\n",
    "hallucinations8 = [\n",
    "    \"Check for incorrect founding year\",\n",
    "    \"Check for wrong founders\",\n",
    "    \"Check for incorrect location\",\n",
    "    \"Check for invented company history details\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Question Answering - Microsoft Founding\",\n",
    "    prompt8,\n",
    "    ground_truth8,\n",
    "    gpt2_output8,\n",
    "    hallucinations8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e619a",
   "metadata": {},
   "source": [
    "---\n",
    "# üéØ DEMONSTRATION 5: Translation & Context Hallucination\n",
    "\n",
    "**Task:** Translate or explain concepts\n",
    "\n",
    "**Risk:** GPT-2 will add interpretations, context, or explanations that are fabricated or incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 9: Technical Term Explanation\n",
    "prompt9 = \"Quantum computing is\"\n",
    "\n",
    "gpt2_output9 = generate_gpt2_text(prompt9, max_length=150, temperature=0.7)\n",
    "\n",
    "ground_truth9 = \"\"\"Quantum computing is a type of computation that uses quantum-mechanical phenomena \n",
    "such as superposition and entanglement to perform operations on data\"\"\"\n",
    "\n",
    "hallucinations9 = [\n",
    "    \"Check for incorrect technical definitions\",\n",
    "    \"Check for invented capabilities or applications\",\n",
    "    \"Check for wrong scientific principles\",\n",
    "    \"Check for fabricated examples or companies\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Technical Explanation - Quantum Computing\",\n",
    "    prompt9,\n",
    "    ground_truth9,\n",
    "    gpt2_output9,\n",
    "    hallucinations9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a39014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 10: Literary Reference\n",
    "prompt10 = \"Shakespeare's Hamlet was written in\"\n",
    "\n",
    "gpt2_output10 = generate_gpt2_text(prompt10, max_length=120, temperature=0.7)\n",
    "\n",
    "ground_truth10 = \"Shakespeare's Hamlet was written between 1599 and 1601, first published in 1603\"\n",
    "\n",
    "hallucinations10 = [\n",
    "    \"Check for incorrect dates\",\n",
    "    \"Check for fabricated plot details\",\n",
    "    \"Check for invented character names\",\n",
    "    \"Check for wrong historical context\"\n",
    "]\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Literary Fact - Hamlet Publication\",\n",
    "    prompt10,\n",
    "    ground_truth10,\n",
    "    gpt2_output10,\n",
    "    hallucinations10\n",
    ")\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### üí° Key Insight #2\n",
    "Across all categories - history, geography, news, medicine, and general knowledge - GPT-2 consistently:\n",
    "- **Generates fluent, convincing text**\n",
    "- **Lacks factual accuracy**\n",
    "- **Cannot verify its own outputs**\n",
    "- **Shows no uncertainty** even when wrong\n",
    "\n",
    "The model's confidence is NOT correlated with correctness!\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bde612",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä Quantitative Analysis: Hallucination Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple generations to measure consistency\n",
    "test_prompts = [\n",
    "    {\"prompt\": \"The capital of France is\", \"correct\": \"Paris\", \"category\": \"Geography\"},\n",
    "    {\"prompt\": \"World War I started in\", \"correct\": \"1914\", \"category\": \"History\"},\n",
    "    {\"prompt\": \"The speed of light is\", \"correct\": \"299,792,458 m/s\", \"category\": \"Science\"},\n",
    "    {\"prompt\": \"The first iPhone was released in\", \"correct\": \"2007\", \"category\": \"Technology\"},\n",
    "    {\"prompt\": \"The largest ocean on Earth is\", \"correct\": \"Pacific Ocean\", \"category\": \"Geography\"},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Running consistency test (3 generations per prompt)...\\n\")\n",
    "\n",
    "for test in test_prompts:\n",
    "    generations = []\n",
    "    for i in range(3):\n",
    "        output = generate_gpt2_text(test['prompt'], max_length=50, temperature=0.7)\n",
    "        generations.append(output)\n",
    "    \n",
    "    # Check if all generations are identical\n",
    "    consistent = len(set(generations)) == 1\n",
    "    \n",
    "    # Check if any contain the correct answer\n",
    "    correct_count = sum(1 for gen in generations if test['correct'].lower() in gen.lower())\n",
    "    \n",
    "    results.append({\n",
    "        'Category': test['category'],\n",
    "        'Prompt': test['prompt'],\n",
    "        'Consistent': 'Yes' if consistent else 'No',\n",
    "        'Correct (out of 3)': correct_count,\n",
    "        'Sample Output': generations[0][:100] + \"...\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "display(df)\n",
    "\n",
    "# Summary statistics\n",
    "consistency_rate = (df['Consistent'] == 'Yes').sum() / len(df) * 100\n",
    "avg_correctness = df['Correct (out of 3)'].mean() / 3 * 100\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### üìà Summary Statistics:\n",
    "- **Consistency Rate:** {consistency_rate:.1f}% (how often 3 generations are identical)\n",
    "- **Average Correctness:** {avg_correctness:.1f}% (how often outputs contain correct answer)\n",
    "\n",
    "**Observation:** Low consistency shows GPT-2 generates different outputs for the same prompt, \n",
    "and low correctness proves it frequently hallucinates incorrect information.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0fbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Hallucination by Category\n",
    "category_data = df.groupby('Category')['Correct (out of 3)'].mean() / 3 * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = category_data.plot(kind='bar', ax=ax, color='#e74c3c', edgecolor='black', linewidth=1.2)\n",
    "ax.set_xlabel('Category', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('GPT-2 Factual Accuracy by Category', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.axhline(y=50, color='red', linestyle='--', label='50% (Random Guess)')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(category_data.values):\n",
    "    ax.text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ùå GPT-2 shows poor factual accuracy across all categories!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e69a6",
   "metadata": {},
   "source": [
    "---\n",
    "# üéì Key Takeaways\n",
    "\n",
    "## What We Proved:\n",
    "1. **GPT-2 frequently hallucinates** - generating plausible but factually incorrect information\n",
    "2. **Hallucinations occur across all domains** - history, geography, science, news, and general knowledge\n",
    "3. **The model shows no uncertainty** - it generates incorrect information with the same confidence as correct information\n",
    "4. **Inconsistent outputs** - the same prompt produces different (often wrong) results\n",
    "\n",
    "## Why This Happens:\n",
    "- **GPT-2 is a statistical model** - it predicts likely word sequences, not facts\n",
    "- **No fact-checking mechanism** - the model cannot verify information against reliable sources\n",
    "- **No knowledge representation** - it doesn't \"know\" facts, it pattern-matches from training data\n",
    "- **No uncertainty awareness** - it can't say \"I don't know\"\n",
    "\n",
    "## Real-World Implications:\n",
    "- ‚ùå **Unreliable for summarization** - adds or changes critical details\n",
    "- ‚ùå **Dangerous for Q&A systems** - provides confident but wrong answers\n",
    "- ‚ùå **Risky for content generation** - spreads misinformation\n",
    "- ‚ùå **Unsuitable for fact-based applications** - medical, legal, financial, news\n",
    "\n",
    "## What's Needed:\n",
    "1. **Fact verification systems** - external knowledge bases and fact-checking\n",
    "2. **Uncertainty quantification** - models should indicate confidence levels\n",
    "3. **Retrieval-augmented generation** - combine language models with search\n",
    "4. **Human oversight** - critical review of AI-generated content\n",
    "5. **User education** - understanding model limitations\n",
    "\n",
    "---\n",
    "\n",
    "## üö® Final Warning\n",
    "\n",
    "> *\"The danger of AI hallucination is not just that it generates false information, but that it does so with such fluency and confidence that humans may trust it without verification. This makes language models particularly risky for applications where factual accuracy matters.\"*\n",
    "\n",
    "**GPT-2 is a powerful text generator - but it is NOT a reliable source of factual information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f2b76",
   "metadata": {},
   "source": [
    "---\n",
    "# üî¨ Interactive Testing Area\n",
    "\n",
    "Use this cell to test your own examples during the live demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own test case!\n",
    "custom_prompt = \"YOUR PROMPT HERE\"\n",
    "custom_ground_truth = \"THE ACTUAL FACTS HERE\"\n",
    "\n",
    "custom_output = generate_gpt2_text(custom_prompt, max_length=150, temperature=0.7)\n",
    "\n",
    "display_hallucination_comparison(\n",
    "    \"Custom Test\",\n",
    "    custom_prompt,\n",
    "    custom_ground_truth,\n",
    "    custom_output,\n",
    "    [\"Check the output for factual accuracy\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
